---
layout: post
title: Visualizing a scikit-learn decision tree
image: /public/images/poker_decision_tree.png
summary: >
  A major advantage of tree-based machine learning methods is that their logic is observable -- you can examine the tree or trees generated by the fitting process and see what decisions it is making to get to an output.
---

Yesterday morning I started playing around with the [poker rule induction challenge](https://www.kaggle.com/c/poker-rule-induction/) on Kaggle. I was surprised to find out the vast majority of five-card stud hand values could be correctly predicted from just two features: the number of unique suits and the number of unique ranks in the hand. I posted as much on Facebook:

<p class="fb-post" data-href="https://www.facebook.com/biesnecker/posts/10103998433224612"></p>

But then I began to wonder how the machine had done it. I had used scikit-learn to build the model, so I started hunting around for a way to visualize the decision tree that the was used to classify the hands. Turns out that it's pretty easy to do, as the base `Tree` module provides a way to export a tree to a [Graphviz](http://www.graphviz.org/) dot file, which can then easily be transformed into a PNG or PDF.

Here's the code:

{% highlight python %}
clf = DecisionTreeClassifier()
clf.fit(x_train, y_train)
with open('tree.dot', 'w') as dotfile:
    export_graphviz(
        clf,
        dotfile,
        feature_names=['unique_suits', 'unique_ranks'])
{% endhighlight %}

I chose to visualize a `DecisionTreeClassifier` instance because I didn't really want to output and examine all `n` many decision trees that make up a random forest (and, with only two features, there weren't very many differences between the trees). If you do, though, it's easy to adapt this code, as `RandomForestClassifier` provides access to the underlying trees.

{% highlight python %}
clf = RandomForestClassifier(n_estimators=10)
clf.fit(x_train, y_train)
for i, tree in enumerate(clf.estimators_):
    with open('tree_' + str(i) + '.dot', 'w') as dotfile:
        export_graphviz(
            clf,
            dotfile,
            feature_names=['unique_suits', 'unique_ranks'])
{% endhighlight %}

You can then use the `dot` utility that ships with Graphviz to turn the resulting dot file into a PNG.

{% highlight bash %}
dot -Tpng tree.dot -o tree.png
{% endhighlight %}

You end up with a gorgeous looking tree like this one (click for the fullsize image):

<a href="/public/images/tree.png"><img src="/public/images/tree.png" alt="DecisionTreeClassifier decision tree" /></a>

The values that are predicted are the hands that appear at that node in the tree.

<ol start="0">
    <li>Nothing in hand</li>
    <li>One pair</li>
    <li>Two pairs</li>
    <li>Three of a kind</li>
    <li>Straight</li>
    <li>Flush</li>
    <li>Full house</li>
    <li>Four of a kind</li>
    <li>Straight flush</li>
    <li>Royal flush</li>
</ol>

Note that any time there are numbers in more than one bucket it means that the tree is going to misclassify some samples -- this tree only gets about 97.5% of hands correct. We can see this pretty easily. If you start at the top node, the first test is are there fewer than five ranks (that is, is there at least one pair): if yes, go to the left, and if no, go to the right. Going right (no), it checks to see if there are fewer than two ranks in the hand, and if yes it classifies the hand as a flush. However, this is only right 84% of the time in the sample data. In the other 16% of the sample data the hand is either a straight flush or a royal flush, neither of which can be differentiated from a vanialla flush by two features we have. In fact, this decision tree <em>never</em> correctly identifies a four of a kind, straight flush, or royal flush, but they don't appear often enough in sample data to knock the overall accuracy of the model down too much.

This is a pretty trivial example and additional features can cause the decision tree to quickly grow in complexity, but the ability to visualize what the machine is doing and understand where additional feature engineering might be helpful is a really useful tool to have in your toolbelt.